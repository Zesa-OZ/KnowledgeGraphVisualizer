services:
  backend:
    build:
      context: ./backend
    ports:
      - "8000:8000"
    env_file:
      - .env.docker
    environment:
      APP_ENV: "docker"
      # Por defecto: demo gratis
      AI_PROVIDER: "openai"
      MONTHLY_BUDGET_USD: "5"
      BUDGET_ENFORCE: "true"
      BUDGET_CACHE_SECONDS: "600"

      # CORS: el frontend en docker se sirve en http://localhost:8080
      FRONTEND_ORIGIN: "http://localhost:8080"

      # Ollama (si lo tienes local fuera de docker)
      OLLAMA_BASE_URL: "http://host.docker.internal:11434"
      OLLAMA_MODEL: "llama3.1"

      # OpenAI (opcional)
      # OPENAI_API_KEY: "${OPENAI_API_KEY:-}"
      # OPENAI_MODEL: "${OPENAI_MODEL:-gpt-4o-mini}"
      # OPENAI_ADMIN_KEY: "${OPENAI_ADMIN_KEY:-}"
      # OPENAI_PROJECT_ID: "${OPENAI_PROJECT_ID:-}"


  frontend:
    build:
      context: ./frontend
    ports:
      - "8080:80"
    depends_on:
      - backend
